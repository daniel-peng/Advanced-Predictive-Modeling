{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Total points: 50 </p>\n",
    "## <p style=\"text-align: center;\">Due: Mon, November 28</p>\n",
    "\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please submit **only one** ipynb file from each group, and include the names of all the group members. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Random Forest vs Boosting - Regression (15pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will compare performance of different ensemble methods for regression problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor). Board game data set from DataQuest will be used (you can download data from Canvas: 'games.csv').\n",
    "\n",
    "1. (1) Load the data, (2) remove duplicate rows, (3) remove features of type string (object in pandas), and (4) replace missing values by mean of each column. Then, partition data into features (X) and the target label (y) for regression task. We want to predict the *average_rating*. Use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. (1pt)\n",
    "\n",
    "2. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to predict average_rating. Find the best parameters (including *n_estimators*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the accuracy of your model in terms of RMSE. (4pts)\n",
    "\n",
    "3. Use [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) for predicting the targets. Again, find the best parameters (including *n_estimators,* and* learning_rate*), and report corresponding RMSE for each algorithm. (8pts)\n",
    "\n",
    "4. Which model did you expect to be more accurate in predicting the targets? Why? Did your observation match this expectation? (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import (RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "game = pd.read_csv('games.csv')\n",
    "\n",
    "#remove duplicate\n",
    "game = game.drop_duplicates()\n",
    "\n",
    "#drop string types\n",
    "game = game.drop('name', 1)\n",
    "game = game.drop('type', 1)\n",
    "game = game.drop('id', 1)\n",
    "\n",
    "#replace missing value\n",
    "game = game.fillna(game.mean())\n",
    "\n",
    "#spliting\n",
    "y = game.average_rating\n",
    "X = game.drop('average_rating', 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 30, 'max_depth': None, 'min_samples_leaf': 3}\n",
      "RMSE:  1.00767396657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "parameters = {\"n_estimators\":[5,10,20,30],\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": ['auto','sqrt','log2'],\n",
    "              \"min_samples_split\": [3, 10],\n",
    "              \"min_samples_leaf\": [3, 10]}\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "clf_rf = GridSearchCV(rf, parameters, cv = 5, scoring=scorer)\n",
    "clf_rf.fit(X_train,y_train)\n",
    "\n",
    "y_best_rf = clf_rf.predict(X_test)\n",
    "RMSE_rf = np.sqrt(mean_squared_error(y_test, y_best_rf))\n",
    "\n",
    "print clf_rf.best_params_\n",
    "print 'RMSE: ', RMSE_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 5, 'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_leaf': 5}\n",
      "RMSE:  0.999426023606\n"
     ]
    }
   ],
   "source": [
    "#GBR\n",
    "gbr = GradientBoostingRegressor(random_state=0, loss='ls')\n",
    "param_grid = {'n_estimators': [10,50,100], \n",
    "              'learning_rate': [0.01, 0.1,1],\n",
    "              'max_depth':[3,7,10],\n",
    "             'min_samples_split':[5,10],\n",
    "             'min_samples_leaf':[5,10]}\n",
    "  \n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "clf_gbr = GridSearchCV(gbr, param_grid, cv = 5, scoring=scorer)\n",
    "clf_gbr.fit(X_train,y_train)\n",
    "\n",
    "y_best_gbr = clf_gbr.predict(X_test)\n",
    "RMSE_gbr = np.sqrt(mean_squared_error(y_test, y_best_gbr))\n",
    "\n",
    "print clf_gbr.best_params_\n",
    "print 'RMSE: ', RMSE_gbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'loss': 'exponential', 'learning_rate': 0.01}\n",
      "RMSE:  1.14109634194\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost\n",
    "ada = AdaBoostRegressor()\n",
    "param_grid = {'n_estimators': [10, 50, 100, 1000], \n",
    "              'learning_rate': [0.001,0.01, 0.1,1,10], \n",
    "             'loss':['linear', 'square', 'exponential']}\n",
    "  \n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "clf_ada = GridSearchCV(ada, param_grid, cv = 5, scoring=scorer)\n",
    "clf_ada.fit(X_train,y_train)\n",
    "\n",
    "y_best_ada = clf_ada.predict(X_test)\n",
    "RMSE_ada = np.sqrt(mean_squared_error(y_test, y_best_ada))\n",
    "\n",
    "print clf_ada.best_params_\n",
    "print 'RMSE: ', RMSE_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "In this case, boosting tree should perform better than Random Forest because we can tune the learning rate and achieve better accuracy. We also suspected that Adaboost wouldn't perform as well as GBR because Adaboost puts high weight on misclassifications, which makes it even more sensitive to outliers. The final results confirmed my hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Random Forest vs Boosting - Classification (15 pts)\n",
    "In this question, we will compare performance of different ensemble methods for classification problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier). [Spam Classification Data](https://archive.ics.uci.edu/ml/datasets/Spambase) of UCI will be used (you can download data from Canvas: 'spam_uci.csv'). Don't worry about column names. The last column represents target label, 1 if spam and zero otherwise.\n",
    "\n",
    "1. Load the data and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. (1pt)\n",
    "\n",
    "2. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to classify whether an email is spam. Find the best parameters (including *n_estimators* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report your testing accuracy ([accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score). You will need [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba) for roc_auc_score. (4pts)\n",
    "\n",
    "3. Use [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for the spam classification problem. Again, find the best parameters (including *n_estimators, learning_rate,* and *max_depth (GBDT only)*), and report corresponding accuracy_score and roc_auc_score on the test data for each algorithm. (8pts)\n",
    "\n",
    "4. Point out one advantage and one disadvantage of Random Forest compared to GBDT (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     0     1     2    3     4     5     6     7     8 ...    48  \\\n",
      "0           0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00 ...  0.00   \n",
      "1           1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00 ...  0.00   \n",
      "2           2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64 ...  0.01   \n",
      "3           3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31 ...  0.00   \n",
      "4           4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31 ...  0.00   \n",
      "\n",
      "      49   50     51     52     53     54   55    56  57  \n",
      "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
      "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
      "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
      "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
      "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "spam = pd.read_csv('spam_uci.csv')\n",
    "print spam.head()\n",
    "\n",
    "#spliting\n",
    "y = spam['57']\n",
    "X = spam.drop('57', 1)\n",
    "X = X.drop('Unnamed: 0', 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 1, 'n_estimators': 30, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 'sqrt', 'max_depth': None}\n",
      "roc score: 0.988833673895\n",
      "accuracy score:  0.953917050691\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "parameters = {'n_estimators':[5,10,20,25,30],\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": ['auto','sqrt','log2',None],\n",
    "              \"min_samples_split\": [3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "clf_rf = GridSearchCV(rf, parameters, cv = 5, scoring=scorer)\n",
    "clf_rf.fit(X_train,y_train)\n",
    "\n",
    "y_best_rf = clf_rf.predict(X_test)\n",
    "rf_prob = np.array([item[1] for item in clf_rf.predict_proba(X_test)])\n",
    "\n",
    "print clf_rf.best_params_\n",
    "print 'roc score:', roc_auc_score(y_test, rf_prob)\n",
    "print 'accuracy score: ', accuracy_score(y_test, y_best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'learning_rate': 0.1, 'max_depth': 3}\n",
      "roc score: 0.95835874174\n",
      "accuracy score:  0.961158657011\n"
     ]
    }
   ],
   "source": [
    "#GBDT\n",
    "gbdt = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0)\n",
    "param_grid = {'n_estimators': [1, 10, 100, 1000], 'learning_rate': [0.001, 0.01, 0.1, 1, 10], \n",
    "              'max_depth': [3, 5, 10,15]}\n",
    "  \n",
    "\n",
    "scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "clf_gbdt = GridSearchCV(gbdt, param_grid, cv = 5, scoring=scorer)\n",
    "clf_gbdt.fit(X_train,y_train)\n",
    "\n",
    "y_best_gbdt = clf_gbdt.predict(X_test)\n",
    "\n",
    "print clf_gbdt.best_params_\n",
    "print 'roc score:', roc_auc_score(y_test, y_best_gbdt)\n",
    "print 'accuracy score: ', accuracy_score(y_test, y_best_gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'learning_rate': 0.1}\n",
      "roc score: 0.986291050178\n",
      "accuracy score:  0.961158657011\n"
     ]
    }
   ],
   "source": [
    "#Adaboost\n",
    "ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\n",
    "param_grid = {'n_estimators': [1, 10, 50, 500], 'learning_rate': [0.01, 0.1, 1, 10, 100]}\n",
    "  \n",
    "\n",
    "scorer = make_scorer(accuracy_score, greater_is_better=True)\n",
    "clf_ada = GridSearchCV(ada, param_grid, cv = 5, scoring=scorer)\n",
    "clf_ada.fit(X_train,y_train)\n",
    "\n",
    "y_best_ada = clf_gbdt.predict(X_test)\n",
    "ada_prob = np.array([item[1] for item in clf_ada.predict_proba(X_test)])\n",
    "\n",
    "print clf_ada.best_params_\n",
    "print 'roc score:', roc_auc_score(y_test, ada_prob)\n",
    "print 'accuracy score: ', accuracy_score(y_test, y_best_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "Advantage: Random Forest is less likely to overfit to the data than GBDT because it's less susceptible to outliers.\n",
    "\n",
    "Disadvantage: GBDT can have better accuracy with fewer trees as it tries to add new trees that compliment the already built ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3 - Matrix Factorization for Rating Prediction (20pts)\n",
    "\n",
    "The movielens dataset contains 1 million movie ratings from several thousand users. We will be using *k*-rank matrix factorization to estimate this dataset as the product $X=UV^T$, where *U* and *V* have only $k$ columns.\n",
    "\n",
    "1) You can download the movielens 1M dataset from https://datahub.io/dataset/movielens, but for this problem use the data available on Canvas. It has been split into training and test sets, and converted to matrix format where the rows correspond to users and the columns to movies. Note that most of the entries are NaNs, indicating that these ratings are missing. An extra file, lens1m_361M_titles.csv, has been added so you can check out specific movies if you're curious.\n",
    "\n",
    "2) Scikit-learn is a little behind for recommender systems, and doesn't have any method to factorize matrices with missing data. Which means you get to code it! Slide 22 of the 'apa large scale learning' lecture notes has the equations for stochastic gradient descent on *U* and *V*. You will have to:\n",
    "* Set up initial guesses for the *U* and *V* matrices. I suggest small random values.\n",
    "* Find a suitable learning rate for the descent. A learning rate that is too large will probably blow up, like in HW3 problem 1.\n",
    "* Come up with a stopping policy\n",
    "* Code the descent algorithm (5 pts)\n",
    "\n",
    "3) Using your SGD algorithm, apply 2-rank matrix factorization on the filled training matrix. Calculate the RMSE of this model on the training data and on the test data (separately). The optimal score on the training data is around .86 RMSE; your version of gradient descent must go at least below .91 RMSE. (5 pts)\n",
    "\n",
    "4) You should notice some overfitting. Because matrix factorization learns separate scores for each movie, a movie with very few reviews may be easily overfit. You may want to only predict ratings when you have enough information to reach a good conclusion. Recalculate the RMSE on the test data, specifically for movies with at least 50 reviews (don't retrain the models). Also report the percent of movies that are still included (after cutting those with < 50 reviews), and the percent of test ratings that are still included. (5 pts)\n",
    "\n",
    "5) Repeat steps 3 and 4 with 5-rank factorization. Display training and test RMSE. (5 pts)\n",
    "\n",
    "Hints:  \n",
    "The numpy function *nanmean* is helpful for RMSE calculation.  \n",
    "The descent algorithm will probably run for at least several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "titles = pd.read_csv('lens1m_361M_titles.csv')\n",
    "test_X = np.load('lens1m_361M_test.npy')\n",
    "train_X = np.load('lens1m_361M_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040L, 3952L)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class sgdMatrixFact():\n",
    "    \"\"\"Train a matrix factorization model to predict empty \n",
    "        entries in a matrix using stochastic gradient descent. \n",
    "        Params:\n",
    "        K (int): number of latent features\n",
    "        n_epochs (int): max number of epochs the model can run while training\n",
    "        learning_rate (float): learning rate for SGD\"\"\"\n",
    "    def __init__(self, K, n_epochs, learning_rate):\n",
    "        self.K = K\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "    \n",
    "    def fit(self,X_train):\n",
    "        \"\"\"The Fit function takes in a NxM matrix and returns 2 matrices, U and V, whose dot product\n",
    "            is the approximation of the original matrix. It also returns a dictionary of the training\n",
    "            RMSEs with each epoch\"\"\"\n",
    "        self.X_train = X_train\n",
    "        n, m = X_train.shape\n",
    "        U = np.random.rand(n,self.K)/1000\n",
    "        V = np.random.rand(m,self.K)/1000\n",
    "        V = V.T #transpose V matrix\n",
    "        \n",
    "        train_rmse = {}\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print \"Working on epoch\",epoch\n",
    "            for i in xrange(len(X_train)):\n",
    "                for j in xrange(len(X_train[i])):\n",
    "                    if X_train[i][j]>0:\n",
    "                        L = X_train[i,j] - np.dot(U[i,:],V[:,j])  # Calculate error for gradient\n",
    "                        for k in range(self.K):\n",
    "                            U[i][k] = U[i][k] + self.learning_rate*(2*L*V[k][j]) # Update latent U feature matrix\n",
    "                            V[k][j] = V[k][j] + self.learning_rate*(2*L*U[i][k]) # Update latent V feature matrix\n",
    "        \n",
    "            X_hat = np.dot(U,V)\n",
    "            rmse = np.sqrt(np.nanmean((X_train - X_hat)**2))\n",
    "            train_rmse[epoch]=rmse\n",
    "            try:\n",
    "                if train_rmse[epoch-1]-rmse <= 0.001:\n",
    "                    break #Stopping rule: break loop if rmse between epochs isn't improving\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        self.train_rmse = train_rmse\n",
    "        self.U = U\n",
    "        self.V = V\n",
    "        V = V.T #transpose V matrix back to normal\n",
    "        return U,V,train_rmse\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        \"\"\"The Predict function uses the U and V matrices derived from the training matrix \n",
    "            and uses it to approximate empty entries in a testing matrix of the same shape. \n",
    "            Returns the approximated matrix and the test RMSE.\"\"\"\n",
    "        X_hat = np.dot(U,V.T)\n",
    "        test_rmse = np.sqrt(np.nanmean((X_test - X_hat)**2))\n",
    "        \n",
    "        self.X_hat = X_hat\n",
    "        self.test_rmse = test_rmse\n",
    "        return X_hat,test_rmse      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n"
     ]
    }
   ],
   "source": [
    "#cross-validating to find an appropriate learning rate\n",
    "learning_rate = np.linspace(0.01,0.001,15)\n",
    "cross_val = {}\n",
    "for l in learning_rate:\n",
    "    mod = sgdMatrixFact(K=2,n_epochs = 5,learning_rate = l)\n",
    "    U,V,rmse =  mod.fit(train_X)\n",
    "    cross_val[l]=rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.001: {0: 3.7502882928430683,\n",
       "  1: 3.4911177639934992,\n",
       "  2: 1.8919557122839699,\n",
       "  3: 1.3159876348936448,\n",
       "  4: 1.1250026815000347},\n",
       " 0.0016428571428571421: {0: 3.5218942176038257,\n",
       "  1: 1.586507688548833,\n",
       "  2: 1.1161458698564171,\n",
       "  3: 1.0054637222580296,\n",
       "  4: 0.96340231743312243},\n",
       " 0.002285714285714285: {0: 2.8222951748141143,\n",
       "  1: 1.1823327751550983,\n",
       "  2: 0.99351126828159275,\n",
       "  3: 0.95144317014240942,\n",
       "  4: 0.93469069699268126},\n",
       " 0.002928571428571428: {0: 2.4833238407006379,\n",
       "  1: 1.0699252584477237,\n",
       "  2: 0.95724239282006096,\n",
       "  3: 0.93580132516690384,\n",
       "  4: 0.92695756243234173},\n",
       " 0.0035714285714285709: {0: 2.4848156798313736,\n",
       "  1: 1.0238114509566973,\n",
       "  2: 0.94360073104318409,\n",
       "  3: 0.93064651498381845,\n",
       "  4: 0.92512742380094037},\n",
       " 0.0042142857142857138: {0: 2.5850792703714514,\n",
       "  1: 1.0012069239775421,\n",
       "  2: 0.9383192214743542,\n",
       "  3: 0.92956146439740017,\n",
       "  4: 0.9257629506615076},\n",
       " 0.0048571428571428567: {0: 2.7524054523501684, 1: inf},\n",
       " 0.0054999999999999997: {0: 2.9236414182641672, 1: inf},\n",
       " 0.0061428571428571426: {0: 2.9728491202679503, 1: inf},\n",
       " 0.0067857142857142855: {0: inf, 1: nan, 2: nan, 3: nan, 4: nan},\n",
       " 0.0074285714285714285: {0: 39928943.978438005,\n",
       "  1: nan,\n",
       "  2: nan,\n",
       "  3: nan,\n",
       "  4: nan},\n",
       " 0.0080714285714285714: {0: inf, 1: nan, 2: nan, 3: nan, 4: nan},\n",
       " 0.0087142857142857143: {0: inf, 1: nan, 2: nan, 3: nan, 4: nan},\n",
       " 0.0093571428571428573: {0: inf, 1: nan, 2: nan, 3: nan, 4: nan},\n",
       " 0.01: {0: inf, 1: nan, 2: nan, 3: nan, 4: nan}}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 5\n",
      "Working on epoch 6\n",
      "Working on epoch 7\n",
      "Working on epoch 8\n",
      "Working on epoch 9\n",
      "Working on epoch 10\n",
      "Working on epoch 11\n",
      "Working on epoch 12\n",
      "Working on epoch 13\n",
      "Working on epoch 14\n",
      "Working on epoch 15\n",
      "Working on epoch 16\n",
      "Working on epoch 17\n"
     ]
    }
   ],
   "source": [
    "#Since learning rate of 0.004 had the best RMSE, we use that, but increase the number of epochs to improve RMSE further.\n",
    "#large number of epochs since stopping rule will take care of it if there are too many\n",
    "mod = sgdMatrixFact(K=2,n_epochs = 20,learning_rate = 0.0042142857142857138) \n",
    "U,V,rmse =  mod.fit(train_X)\n",
    "x_hat,test_rmse = mod.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2.5853193316908043, 1: 1.0011853781428373, 2: 0.93829741038596226, 3: 0.92952104296837912, 4: 0.925667316158444, 5: 0.92346245989240028, 6: 0.92167854163449592, 7: 0.91930932248581798, 8: 0.91564915684542592, 9: 0.91179442733142468, 10: 0.90900714891806744, 11: 0.90693997032273599, 12: 0.90524428415102121, 13: 0.90380068727552376, 14: 0.90254073581113137, 15: 0.90141054372764129, 16: 0.90036559843608943, 17: 0.89937042947662149}\n",
      "0.924827123784\n"
     ]
    }
   ],
   "source": [
    "print rmse\n",
    "print test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_use=[]\n",
    "for col in range(test_X.shape[1]):\n",
    "    if len(test_X[:,col][np.isfinite(test_X[:,col])])>=50:\n",
    "        cols_to_use.append(col)\n",
    "\n",
    "test_X_50 = test_X[:,cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for Movies with over 50 reviews: 0.915069739888\n"
     ]
    }
   ],
   "source": [
    "x_hat_50 = x_hat[:,cols_to_use]\n",
    "print \"Test RMSE for Movies with over 50 reviews:\", np.sqrt(np.nanmean((test_X_50 - x_hat_50)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of movies still included: 30.79%\n",
      "Percent of ratings still included: 81.19%\n"
     ]
    }
   ],
   "source": [
    "print \"Percent of movies still included: {:.2f}%\".format((float(test_X_50.shape[1])/test_X.shape[1])*100)\n",
    "print \"Percent of ratings still included: {:.2f}%\".format((float(len(test_X_50[np.isfinite(test_X_50)]))/len(test_X[np.isfinite(test_X)]))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epoch 0\n",
      "Working on epoch 1\n",
      "Working on epoch 2\n",
      "Working on epoch 3\n",
      "Working on epoch 4\n",
      "Working on epoch 5\n",
      "Working on epoch 6\n",
      "Working on epoch 7\n",
      "Working on epoch 8\n",
      "Working on epoch 9\n",
      "Working on epoch 10\n",
      "Working on epoch 11\n",
      "Working on epoch 12\n",
      "Working on epoch 13\n",
      "Working on epoch 14\n",
      "Working on epoch 15\n",
      "Working on epoch 16\n",
      "Working on epoch 17\n",
      "Working on epoch 18\n",
      "Working on epoch 19\n"
     ]
    }
   ],
   "source": [
    "mod1 = sgdMatrixFact(K=5,n_epochs = 20,learning_rate = 0.0042142857142857138) \n",
    "U1,V1,rmse1 =  mod.fit(train_X)\n",
    "x_hat1,test_rmse1 = mod.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2.5854680896037365, 1: 1.001295715580536, 2: 0.93829029998453817, 3: 0.92947816142040263, 4: 0.92553947385733237, 5: 0.92308247191871229, 6: 0.92059986910925562, 7: 0.91694246162152893, 8: 0.91254701585407461, 9: 0.90900427414003504, 10: 0.90624060918426452, 11: 0.90383807363295277, 12: 0.90167623333885671, 13: 0.89970543731292496, 14: 0.89788810789611018, 15: 0.89619784620985066, 16: 0.89462248907506814, 17: 0.89316249437276285, 18: 0.89182549450167592, 19: 0.89062017576247843}\n",
      "0.924827123784\n"
     ]
    }
   ],
   "source": [
    "print rmse1\n",
    "print test_rmse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for Movies with over 50 reviews: 0.915069739888\n"
     ]
    }
   ],
   "source": [
    "x_hat1_50 = x_hat1[:,cols_to_use]\n",
    "print \"Test RMSE for Movies with over 50 reviews:\", np.sqrt(np.nanmean((test_X_50 - x_hat1_50)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
